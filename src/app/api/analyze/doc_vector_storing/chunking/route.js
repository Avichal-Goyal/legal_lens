import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { NextResponse } from "next/server";

function cleanLegalChunk(text) {
    return String(text)
        .replace(/\s+/g, ' ')
        .replace(/[^a-zA-Z0-9\s,.\-\[\]]/g, '')
        .trim()
}


export async function POST(request) {

    // starting to implement the RAG logic in making chunks of the docs and storing them in a way so to make vector search possible

    const text = await request.json().then(data => data.text);

    // creating a text splitter object



    const splitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 200,
    });
    // chunkOverlap is for the case if any sentence is cut in half, so when it considers the other half of the sentence for the next chunk it will include the first half of that sentence in the new chunk again so that the AI does not loose the context of the document or para.

    // creating chunks
    // this takes the rawText and returns an array of Document Objects
    const chunks = await splitter.createDocuments([text]);

    // checking how many chunks are generated by this
    console.log(`Original Text Length: ${text.length}`);
    console.log(`Number of Chunks: ${chunks.length}`)

    // sanitizing the chunks (4 level sanitization )
    let lastSeenPage = "Unknown";
    const cleanChunks = chunks.map(doc => {
        let text = doc.pageContent;

        const pageNumMatch = text.match(/--- PAGE (\d+) ---/);

        if (pageNumMatch) {
            lastSeenPage = pageNumMatch[1];
        } else {
            text = `--- PAGE ${lastSeenPage} (cont.) --- ${text}`;
        }

        return cleanLegalChunk(text);
    })
    .filter(chunk => chunk.length > 50)

    // sending these chunks to my Vector Database

    return NextResponse.json({
        data: cleanChunks,
        message: "Chunks created successfully",
        count: chunks.length
    });
}